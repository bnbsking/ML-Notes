{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from sklearn.metrics import f1_score, average_precision_score, classification_report\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve\n",
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class History:\n",
    "    def __init__(self, save_results:str):\n",
    "        self.train_loss, self.train_f1, self.train_aps, self.train_map = [], [], [], []\n",
    "        self.valid_loss, self.valid_f1, self.valid_aps, self.valid_map = [], [], [], []\n",
    "        self.save_results = save_results\n",
    "\n",
    "    def __repr__(self):\n",
    "        D = {}\n",
    "        for key in ['train_loss', 'train_f1', 'train_map', 'valid_loss', 'valid_f1', 'valid_map']:\n",
    "            D[key] = round(getattr(self,key)[-1],4) if len(getattr(self,key)) else np.nan\n",
    "        return str(D)\n",
    "        \n",
    "    def save(self, mode='train'):\n",
    "        results = {key:getattr(self,key) for key in dir(self) if 'train_' in key or 'valid_' in key}\n",
    "        json.dump(results, open(os.path.join(self.save_results, f'history_{mode}.json'),'w'))\n",
    "\n",
    "\n",
    "class ComputeMetrics:\n",
    "    \"\"\"\n",
    "    label: np.array[int], shape=(N,)\n",
    "    pred_probs: np.array[float], shape=(N, cls)\n",
    "    # for single score -> concat 1-p and 1-p first\n",
    "    # for unbounded score -> normalize first\n",
    "    \"\"\"\n",
    "    def __init__(self, label, pred_probs, threshold_optimization=False):\n",
    "        self.label = label\n",
    "        self.pred_probs = pred_probs\n",
    "        self.classes = pred_probs.shape[-1]\n",
    "        if not threshold_optimization:\n",
    "            self.pred_cls = pred_probs.argmax(axis=1)\n",
    "            print(f\"\\ndefault_threshold={1/self.classes:.4f}\")\n",
    "        else:\n",
    "            best_threshold = self.threshold_optimization()\n",
    "            print(f\"\\nbest_threshold={best_threshold:.4f}\")\n",
    "            self.pred_cls = np.array([ row[:-1].argmax() if row.max()>=best_threshold else self.classes-1 \\\n",
    "                for row in self.pred_probs ])\n",
    "\n",
    "    def threshold_optimization(self, strategy='f1'):\n",
    "        best_threshold_cls = []\n",
    "        for i in range(self.classes-1):\n",
    "            precision, recall, thresholds = precision_recall_curve(self.label==i, self.pred_probs[:,i])\n",
    "            if strategy=='f1':\n",
    "                f1 = np.array([ 2*p*r/(p+r) if p+r else 0 for p,r in zip(precision,recall) ])\n",
    "                best_threshold_cls.append( thresholds[f1.argmax()] )\n",
    "        return sum(best_threshold_cls)/(self.classes-1)\n",
    "\n",
    "    def get_f1(self):\n",
    "        return f1_score(self.label, self.pred_cls, average='macro')\n",
    "    \n",
    "    def get_aps(self):\n",
    "        aps = [ average_precision_score(self.label==i, self.pred_probs[:,i]) for i in range(self.classes) ]\n",
    "        return aps\n",
    "\n",
    "    def get_cls_report(self):\n",
    "        return classification_report(self.label, self.pred_cls, zero_division=0)\n",
    "\n",
    "    def get_aucs_specificities(self):\n",
    "        aucs, specificities = [], []\n",
    "        for i in range(self.classes):\n",
    "            aucs.append( roc_auc_score(self.label==i, self.pred_probs[:,i]) )\n",
    "            fpr, tpr, thresholds = roc_curve(self.label==i, self.pred_probs[:,i])\n",
    "            specificities.append( 1-fpr.mean() )\n",
    "        return aucs, specificities\n",
    "    \n",
    "    def get_confusion(self, path_list=[], losses=[]):\n",
    "        confusion = [ [ [] for _ in range(self.classes) ] for _ in range(self.classes) ]\n",
    "        path_list = path_list if path_list else ['']*len(self.label)\n",
    "        losses = losses if losses else [-1]*len(self.label)\n",
    "        for gt, pdc, path, loss in zip(self.label, self.pred_cls, path_list, losses):\n",
    "            confusion[gt][pdc].append( (loss,path) )\n",
    "        confusion_cnt = [ [ len(confusion[i][j]) for j in range(self.classes) ] for i in range(self.classes) ]\n",
    "        return confusion, confusion_cnt\n",
    "    \n",
    "    def export_confusion(self, confusion, output_path, top_n=5):\n",
    "        for i in range(self.classes):\n",
    "            for j in range(self.classes):\n",
    "                if i==j: continue\n",
    "                grid_path = os.path.join(output_path, 'confusion', f\"gt_{i}_pd_{j}\")\n",
    "                for _, path in sorted(confusion[i][j])[:top_n]:\n",
    "                    os.makedirs(grid_path, exist_ok=True)\n",
    "                    shutil.copy(path, grid_path)\n",
    "\n",
    "    def export_lowest_conf(self, path_list, output_path, top_n=5):\n",
    "        prob_path_list = sorted(zip(self.pred_probs.max(axis=1), path_list))\n",
    "        worst_path = f\"{output_path}/worst_imgs\"\n",
    "        os.makedirs(worst_path, exist_ok=True)\n",
    "        for _, path in prob_path_list[:top_n]:\n",
    "            shutil.copy(path, worst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from torchmetrics.functional import precision_recall_curve\n",
    "# precision_recall_curve(preds, target)\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "# precision_recall_curve(y_true, y_pred)\n",
    "\n",
    "e.g. preds = [0.1, 0.5, 0.7, 0.5], target = [0, 1, 0, 1]\n",
    "1 sort unique preds | thresholds = [0.1, 0.5, 0.7]\n",
    "2 remove leading 0-label elements | thresholds = [0.5, 0.7]\n",
    "    (must exist better higher threshold, recall unchange but precision increase)\n",
    "3 compute based on 'precision[i]>=threshold[i]'\n",
    "    precision = [0.666, 0, 1]\n",
    "    recall = [1, 0, 0]\n",
    "4 add final p=1, r=0\n",
    "    precision = [0.666, 0, 1]\n",
    "    recall = [1, 0, 0, 0]\n",
    "    thresholds = [0.5, 0.7]\n",
    "    \n",
    "f1[0] = f(precision[0]=class_1_num, recall[0]=1). If class balance f1[0]=2*0.5*1/(0.5+1)=0.666\n",
    "might be higher than random data. So best f1 thresholds might be thresholds[0] most the times.\n",
    "If we don't want best_threshold = threshold[0]:\n",
    "    + lower the class-1 samples (decrease f1[0])\n",
    "    + set high prob label as class-1 (increase f1[others])\n",
    "\"\"\"\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
