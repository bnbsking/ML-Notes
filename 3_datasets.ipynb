{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from torchvision import transforms\n",
    "self.transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ColorJitter([0.7,1.3], [0.7,1.3], [0.7,1.3], [-0.5,0.5]),\n",
    "    # brigntness, contrast, saturation, hue\n",
    "    transforms.RandomRotation([-90,90]),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    # RGB mean & std based on imagenet\n",
    "])\n",
    "\"\"\"\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.6003, 0.9678],\n",
      "         [0.7393, 0.3976]]]), tensor(0))\n"
     ]
    }
   ],
   "source": [
    "# 1. customized class\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x = [ torch.rand(1,2,2) for i in range(4) ]\n",
    "        self.y = torch.tensor([ i%2 for i in range(4) ], dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "dataset = MyDataset()\n",
    "print( next(iter(dataset)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 <class 'torchvision.datasets.folder.ImageFolder'>\n",
      "['cls1', 'cls2']\n",
      "('./_data/example_img_folder/cls1/0.jpg', 0) 0\n",
      "(<PIL.Image.Image image mode=RGB size=2x2 at 0x7FE79528A2E0>, 0)\n"
     ]
    }
   ],
   "source": [
    "# 2. from folder (image classification)\n",
    "os.makedirs(\"./_data/example_img_folder/cls1\", exist_ok=True)\n",
    "for i in range(2):\n",
    "    img = np.random.randint(0, 256, size=(2,2,1), dtype=np.uint8)\n",
    "    cv2.imwrite(f\"./_data/example_img_folder/cls1/{i}.jpg\", img)\n",
    "\n",
    "os.makedirs(\"./_data/example_img_folder/cls2\", exist_ok=True)\n",
    "for i in range(2,4):\n",
    "    img = np.random.randint(0, 256, size=(2,2,1), dtype=np.uint8)\n",
    "    cv2.imwrite(f\"./_data/example_img_folder/cls2/{i}.jpg\", img)\n",
    "\n",
    "dataset = ImageFolder(\"./_data/example_img_folder\")\n",
    "print( len(dataset), type(dataset) )\n",
    "print( dataset.classes )\n",
    "print( dataset.imgs[0], dataset.targets[0] )\n",
    "print( next(iter(dataset)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 2, 2]) torch.Size([4])\n",
      "(tensor([[[0.0494, 0.2581],\n",
      "         [0.4403, 0.8102]]]), tensor(0))\n"
     ]
    }
   ],
   "source": [
    "# 3. from tensor\n",
    "x = torch.rand(4,1,2,2)\n",
    "y = torch.tensor([ i%2 for i in range(4) ], dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(x, y)\n",
    "print( dataset.tensors[0].shape, dataset.tensors[1].shape )\n",
    "print( next(iter(dataset)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader(dataset, *)\n",
    "+ batch_size: int\n",
    "+ collate_fn: func\n",
    "+ pin_memory: bool (faster or not)\n",
    "+ drop_last: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1, 1, 1],\n",
      "        [2, 2, 2]]), tensor([1, 2])]\n",
      "\n",
      "(tensor([[1, 1, 1],\n",
      "        [2, 2, 2]]), tensor([1, 2]))\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,1,1], [2,2,2], [3,3,3], [4,4,4]])\n",
    "y = torch.tensor([1,2,3,4])\n",
    "dataset = TensorDataset(x,y)\n",
    "\n",
    "# 1 default collate_fn\n",
    "print( next(iter(DataLoader(dataset, batch_size=2))) )\n",
    "print()\n",
    "\n",
    "# 2 customized collate_fn\n",
    "def my_collate(batch):\n",
    "    x_list, y_list = [], []\n",
    "    for x, y in batch: # format same as iterating dataset\n",
    "        x_list.append(x), y_list.append(y)\n",
    "    return torch.stack(x_list), torch.stack(y_list)\n",
    "print( next(iter(DataLoader(dataset, batch_size=2, collate_fn=my_collate))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torchvision.datasets\n",
    "\n",
    "+ https://pytorch.org/vision/main/datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "354M\t./_data/cifar10\n",
      "50000 10000 <class 'torchvision.datasets.cifar.CIFAR10'>\n",
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "10000 (32, 32, 3)\n",
      "10000 [3, 8, 8, 0, 6]\n",
      "(<PIL.Image.Image image mode=RGB size=32x32 at 0x7FEFE2892B80>, 3)\n"
     ]
    }
   ],
   "source": [
    "# 1. CIFAR10\n",
    "# download=True means download all # train=True/False means return train/val part\n",
    "trainset = CIFAR10(root=\"./_data/cifar10\", train=True, download=True)\n",
    "validset = CIFAR10(root=\"./_data/cifar10\", train=False, download=True)\n",
    "!du -sh ./_data/cifar10\n",
    "\n",
    "print( len(trainset), len(validset), type(validset) )\n",
    "print( validset.classes )\n",
    "print( len(validset.data), validset.data[0].shape )\n",
    "print( len(validset.targets), validset.targets[:5] )\n",
    "print( next(iter(validset)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampler\n",
    "+ keyword of DataLoader \"shuffle\" and \"sampler\" cannot be both specified\n",
    "    + shuffle=True <-> sampler = SequentialSampler(dataset)\n",
    "    + shuffle=False <-> sampler = RandomSampler(dataset, replacement=False) / sampler = SubsetRandomSampler(indices)\n",
    "+ Unbalanced -> sampler = WeightedRandomSampler(weights, num_samples=len(dataset), replacement=True)\n",
    "+ Custom Sampler -> Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import SequentialSampler, RandomSampler, SubsetRandomSampler, WeightedRandomSampler, BatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NullDataset(Dataset):\n",
    "    def __len__(self):\n",
    "        return 5\n",
    "    def __getitem__(self, index):\n",
    "        return 0\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, size):\n",
    "        self.x = torch.arange(size)\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index]\n",
    "    \n",
    "dataset = MyDataset(7)\n",
    "nullset = NullDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent class\n",
    "if 0 and \"source code\":\n",
    "    class Sampler(object):\n",
    "        r\"\"\"Base class for all Samplers.\n",
    "        Every Sampler subclass has to provide an __iter__ method, providing a way\n",
    "        to iterate over indices of dataset elements, and a __len__ method that\n",
    "        returns the length of the returned iterators.\n",
    "        \"\"\"\n",
    "        # 一个 迭代器 基类\n",
    "        def __init__(self, data_source):\n",
    "            pass\n",
    "\n",
    "        def __iter__(self):\n",
    "            raise NotImplementedError\n",
    "\n",
    "        def __len__(self):\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0])]\n",
      "[tensor([0, 1]), tensor([2, 3]), tensor([4])]\n",
      "[tensor([0, 1]), tensor([2, 3]), tensor([4, 5]), tensor([6])] \n",
      "\n",
      "[tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0])]\n",
      "[tensor([0, 4]), tensor([1, 2]), tensor([3])]\n",
      "[tensor([0, 4]), tensor([1, 2]), tensor([5, 3]), tensor([6])]\n"
     ]
    }
   ],
   "source": [
    "# sequential sampler: 1st arg refers __getitem__, SequentialSampler refers __len__\n",
    "if 0 and \"source code\":\n",
    "    class SequentialSampler(Sampler[int]):\n",
    "        r\"\"\"Samples elements sequentially, always in the same order.\n",
    "\n",
    "        Args:\n",
    "            data_source (Dataset): dataset to sample from\n",
    "        \"\"\"\n",
    "        data_source: Sized\n",
    "\n",
    "        def __init__(self, data_source: Sized) -> None:\n",
    "            self.data_source = data_source\n",
    "\n",
    "        def __iter__(self) -> Iterator[int]:\n",
    "            return iter(range(len(self.data_source)))\n",
    "\n",
    "        def __len__(self) -> int:\n",
    "            return len(self.data_source)\n",
    "        \n",
    "loader = torch.utils.data.DataLoader(nullset, batch_size=2, sampler=SequentialSampler(dataset))\n",
    "print([data for data in loader])\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=2, sampler=SequentialSampler(nullset))\n",
    "print([data for data in loader])\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=2, sampler=SequentialSampler(dataset)) # real case used\n",
    "print([data for data in loader], \"\\n\")\n",
    "\n",
    "# random sampler\n",
    "if 0 and \"source code\":\n",
    "    class RandomSampler(Sampler):\n",
    "        r\"\"\"Samples elements randomly. If without replacement, then sample from a shuffled dataset.\n",
    "        If with replacement, then user can specify ``num_samples`` to draw.\n",
    "        Arguments:\n",
    "            data_source (Dataset): dataset to sample from\n",
    "            num_samples (int): number of samples to draw, default=len(dataset)\n",
    "            replacement (bool): samples are drawn with replacement if ``True``, default=False\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, data_source, replacement=False, num_samples=None):\n",
    "            self.data_source = data_source\n",
    "            self.replacement = replacement\n",
    "            self.num_samples = num_samples\n",
    "\n",
    "            if self.num_samples is not None and replacement is False:\n",
    "                raise ValueError(\"With replacement=False, num_samples should not be specified, \"\n",
    "                                \"since a random permute will be performed.\")\n",
    "\n",
    "            if self.num_samples is None:\n",
    "                self.num_samples = len(self.data_source)\n",
    "\n",
    "            if not isinstance(self.num_samples, int) or self.num_samples <= 0:\n",
    "                raise ValueError(\"num_samples should be a positive integeral \"\n",
    "                                \"value, but got num_samples={}\".format(self.num_samples))\n",
    "            if not isinstance(self.replacement, bool):\n",
    "                raise ValueError(\"replacement should be a boolean value, but got \"\n",
    "                                \"replacement={}\".format(self.replacement))\n",
    "\n",
    "        def __iter__(self):\n",
    "            n = len(self.data_source)\n",
    "            if self.replacement:\n",
    "                return iter(torch.randint(high=n, size=(self.num_samples,), dtype=torch.int64).tolist())\n",
    "            return iter(torch.randperm(n).tolist())\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data_source)\n",
    "        \n",
    "loader = torch.utils.data.DataLoader(nullset, batch_size=2, sampler=RandomSampler(dataset))\n",
    "print([data for data in loader])\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=2, sampler=RandomSampler(nullset))\n",
    "print([data for data in loader])\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=2, sampler=RandomSampler(dataset)) # real case used\n",
    "print([data for data in loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1, 0]), tensor([2, 3]), tensor([4])]\n",
      "[tensor([6, 5])]\n"
     ]
    }
   ],
   "source": [
    "# SubsetRandomSampler\n",
    "if 0 and \"source code\":\n",
    "    class SubsetRandomSampler(Sampler):\n",
    "        r\"\"\"Samples elements randomly from a given list of indices, without replacement.\n",
    "        Arguments:\n",
    "            indices (sequence): a sequence of indices\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, indices):\n",
    "            self.indices = indices\n",
    "\n",
    "        def __iter__(self):\n",
    "            return (self.indices[i] for i in torch.randperm(len(self.indices)))\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.indices)\n",
    "\n",
    "indices = list(range(len(dataset))) # can shuffle or not\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=2, sampler=SubsetRandomSampler(indices[:5]))\n",
    "print([data for data in loader])\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=2, sampler=SubsetRandomSampler(indices[5:]))\n",
    "print([data for data in loader])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0, 0]), tensor([0, 0]), tensor([1, 0]), tensor([0])]\n"
     ]
    }
   ],
   "source": [
    "# WeightedRandomSampler\n",
    "if 0 and \"source code\":\n",
    "    class WeightedRandomSampler(Sampler):\n",
    "        r\"\"\"Samples elements from [0,..,len(weights)-1] with given probabilities (weights).\n",
    "        Arguments:\n",
    "            weights (sequence)   : a sequence of weights, not necessary summing up to one\n",
    "            num_samples (int): number of samples to draw\n",
    "            replacement (bool): if ``True``, samples are drawn with replacement.\n",
    "                If not, they are drawn without replacement, which means that when a\n",
    "                sample index is drawn for a row, it cannot be drawn again for that row.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, weights, num_samples, replacement=True):\n",
    "            if not isinstance(num_samples, _int_classes) or isinstance(num_samples, bool) or \\\n",
    "                    num_samples <= 0:\n",
    "                raise ValueError(\"num_samples should be a positive integeral \"\n",
    "                                \"value, but got num_samples={}\".format(num_samples))\n",
    "            if not isinstance(replacement, bool):\n",
    "                raise ValueError(\"replacement should be a boolean value, but got \"\n",
    "                                \"replacement={}\".format(replacement))\n",
    "            self.weights = torch.tensor(weights, dtype=torch.double)\n",
    "            self.num_samples = num_samples\n",
    "            self.replacement = replacement\n",
    "\n",
    "        def __iter__(self):\n",
    "            return iter(torch.multinomial(self.weights, self.num_samples, self.replacement).tolist())\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.num_samples  ## 指的是一次一共采样的样本的数量\n",
    "\n",
    "weights = [5, 1, 0, 0, 0, 0]\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=2, sampler=WeightedRandomSampler(weights, num_samples=len(dataset), replacement=True))\n",
    "print([data for data in loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6])]\n"
     ]
    }
   ],
   "source": [
    "# BatchSampler\n",
    "weights = [5, 1, 0, 0, 0, 0]\n",
    "batch_sampler = BatchSampler(SequentialSampler(dataset), batch_size=3, drop_last=False)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_sampler=batch_sampler)\n",
    "print([data for data in loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional (preprocessed) Sampling\n",
    "+ Undersampling\n",
    "    + CC, CNN, ENN, Near Miss, Tomek links, OSS\n",
    "+ Oversampling\n",
    "    + SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "from imblearn.under_sampling import ClusterCentroids, CondensedNearestNeighbour, EditedNearestNeighbours,\\\n",
    "    NearMiss, TomekLinks, OneSidedSelection\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> (1000, 20) (1000,)\n",
      "Original dataset shape Counter({1: 900, 0: 100})\n",
      "\n",
      "0.2863 <class 'numpy.ndarray'> <class 'numpy.ndarray'> (200, 20) (200,)\n",
      "Resampled dataset shape Counter({0: 100, 1: 100})\n",
      "\n",
      "0.6952 <class 'numpy.ndarray'> <class 'numpy.ndarray'> (144, 20) (144,)\n",
      "Resampled dataset shape Counter({0: 100, 1: 44})\n",
      "\n",
      "0.0062 <class 'numpy.ndarray'> <class 'numpy.ndarray'> (987, 20) (987,)\n",
      "Resampled dataset shape Counter({1: 887, 0: 100})\n",
      "\n",
      "0.0438 <class 'numpy.ndarray'> <class 'numpy.ndarray'> (200, 20) (200,)\n",
      "Resampled dataset shape Counter({0: 100, 1: 100})\n",
      "\n",
      "0.0061 <class 'numpy.ndarray'> <class 'numpy.ndarray'> (997, 20) (997,)\n",
      "Resampled dataset shape Counter({1: 897, 0: 100})\n",
      "\n",
      "0.0493 <class 'numpy.ndarray'> <class 'numpy.ndarray'> (596, 20) (596,)\n",
      "Resampled dataset shape Counter({1: 496, 0: 100})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y = make_classification(n_classes=2, class_sep=2,\n",
    "    weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n",
    "    n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10) # generate numpy dataset\n",
    "print(type(X), type(y), X.shape, y.shape)\n",
    "print('Original dataset shape %s\\n' % Counter(y)) # Original dataset shape Counter({1: 900, 0: 100})\n",
    "\n",
    "cc = ClusterCentroids(\n",
    "    estimator=MiniBatchKMeans(n_init=1, random_state=0), random_state=42\n",
    ") # default strategy is downsampling # n_init: cluster centers \n",
    "start = time.time()\n",
    "X_res, y_res = cc.fit_resample(X, y)\n",
    "print(round(time.time()-start, 4), type(X_res), type(y_res), X_res.shape, y_res.shape)\n",
    "print('Resampled dataset shape %s\\n' % Counter(y_res))\n",
    "\n",
    "cnn = CondensedNearestNeighbour(random_state=42) # n_neighbors==1\n",
    "start = time.time()\n",
    "X_res, y_res = cnn.fit_resample(X, y)\n",
    "print(round(time.time()-start, 4), type(X_res), type(y_res), X_res.shape, y_res.shape)\n",
    "print('Resampled dataset shape %s\\n' % Counter(y_res))\n",
    "\n",
    "enn = EditedNearestNeighbours() # n_neighbors=3, kind_sel='all'\n",
    "start = time.time()\n",
    "X_res, y_res = enn.fit_resample(X, y)\n",
    "print(round(time.time()-start, 4), type(X_res), type(y_res), X_res.shape, y_res.shape)\n",
    "print('Resampled dataset shape %s\\n' % Counter(y_res))\n",
    "\n",
    "nm = NearMiss()\n",
    "start = time.time()\n",
    "X_res, y_res = nm.fit_resample(X, y)\n",
    "print(round(time.time()-start, 4), type(X_res), type(y_res), X_res.shape, y_res.shape)\n",
    "print('Resampled dataset shape %s\\n' % Counter(y_res))\n",
    "\n",
    "tl = TomekLinks()\n",
    "start = time.time()\n",
    "X_res, y_res = tl.fit_resample(X, y)\n",
    "print(round(time.time()-start, 4), type(X_res), type(y_res), X_res.shape, y_res.shape)\n",
    "print('Resampled dataset shape %s\\n' % Counter(y_res))\n",
    "\n",
    "oss = OneSidedSelection(random_state=42) # n_neighbors=1\n",
    "start = time.time()\n",
    "X_res, y_res = oss.fit_resample(X, y)\n",
    "print(round(time.time()-start, 4), type(X_res), type(y_res), X_res.shape, y_res.shape)\n",
    "print('Resampled dataset shape %s\\n' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> (1000, 20) (1000,)\n",
      "Original dataset shape Counter({1: 900, 0: 100})\n",
      "0.0027 <class 'numpy.ndarray'> <class 'numpy.ndarray'> (1800, 20) (1800,)\n",
      "Resampled dataset shape Counter({0: 900, 1: 900})\n"
     ]
    }
   ],
   "source": [
    "X, y = make_classification(n_classes=2, class_sep=2,\n",
    "    weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n",
    "    n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "print(type(X), type(y), X.shape, y.shape)\n",
    "print('Original dataset shape %s' % Counter(y))\n",
    "\n",
    "sm = SMOTE(random_state=42) # k_neighbors=5\n",
    "start = time.time()\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "print(round(time.time()-start, 4), type(X_res), type(y_res), X_res.shape, y_res.shape)\n",
    "print('Resampled dataset shape %s' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> (1000, 20) (1000,)\n",
      "Original dataset shape Counter({0: 900, 1: 100})\n",
      "\n",
      "0.2328 <class 'numpy.ndarray'> <class 'numpy.ndarray'> (300, 20) (300,)\n",
      "Resampled dataset shape Counter({0: 200, 1: 100})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y = make_classification(n_classes=2, class_sep=2,\n",
    "    weights=[0.9, 0.1], n_informative=3, n_redundant=1, flip_y=0,\n",
    "    n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10) # generate numpy dataset\n",
    "print(type(X), type(y), X.shape, y.shape)\n",
    "print('Original dataset shape %s\\n' % Counter(y)) # Original dataset shape Counter({1: 900, 0: 100})\n",
    "\n",
    "cc = ClusterCentroids(\n",
    "    sampling_strategy=0.5,\n",
    "    estimator=MiniBatchKMeans(n_init=1, random_state=0), random_state=42\n",
    ") # default strategy is downsampling # n_init: cluster centers \n",
    "start = time.time()\n",
    "X_res, y_res = cc.fit_resample(X, y)\n",
    "print(round(time.time()-start, 4), type(X_res), type(y_res), X_res.shape, y_res.shape)\n",
    "print('Resampled dataset shape %s\\n' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "799be347dad3fb91990aadbc2462afb3a707327f285dec2ac2cfd2ae7f9fe191"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
